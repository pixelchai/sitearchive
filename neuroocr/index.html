<p>The aim of the NeuroOCR project is to produce an artificial neural network capable of learning to recognise different characters (the lowercase and uppercase alphabet as well as the numbers), i.e., approximate the correct 6 Bit binary representation of a hand drawn character inputted as a bitmap image. In achieving this, create a versatile Multi-Layered Neural Network library and implement a supervised backpropagation artificial neural network. Or to put it simply, teach a computer to read.</p>

<p>First of all, to understand how this program works, you must understand the beauty of the supervised backpropagtion neural network. Let me give you a basic introduction to neural networks.</p>

<h2 id="what-is-a-neural-network">What is a Neural Network?</h2>

<p>In simple terms, a Supervised Artificial Neural Network is a model, inspired by biological neural networks, with which you can “teach” a computer to approximate a function by “learning” from datasets of inputs and their corresponding outputs (these inputs and outputs are generally 1 or 0 values). In other words, given inputs and their corresponding outputs, the neural network can “learn” to approximate the function that results in those outputs from those inputs.</p>

<h2 id="how-do-they-work">How do they Work?</h2>
<p>An artificial neural network consists of neurons that are arranged in neuron layers. They can be visualised as shown in the diagram (you can have multiple hidden layers):
<img src="/assets/projstuff/neuroocr/diagram.png" /></p>
<center><small>Image source: MDPI AG (Basel, Switzerland) <a href="http://www.mdpi.com/2073-4441/7/8/4232/htm">link to paper</a></small></center>

<p>Each neuron <span class="k1"></span> has a number of inputs and outputs. The inputs to that neuron are modified by a weight <span class="k2"></span> which can reduce or increase the input value (these weights start off as small, random values – this is to increase neural network training efficiency). To calculate the output of a particular neuron, all of the modified inputs to a the neuron are summed, resulting in the internal value <span class="k3"></span>, which is then biased using an Activation Function (<span class="k4"></span>). The activation function, in my case, is the sigmoid function <span class="k5"></span>. Another commonly used activation function is the threshold function, <span class="k6"></span>
<script>
katex.render("X_n", $('.k1')[0]);
katex.render("W_{nj}", $('.k2')[0]);
katex.render("U_j", $('.k3')[0]);
katex.render("\\varphi", $('.k4')[0]);
katex.render("f(x)=\\frac1{1+e^{-\\beta x}}", $('.k5')[0]);
katex.render("f(x)=\\left\\{\\begin{array}{l}0\\;if\\;0\&gt;x\\\\1\\;if\\;x\\geqslant0\\end{array}\\right.", $('.k6')[0]);
</script></p>
